{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom Churn - ML Group Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    " \n",
    "For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    " \n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    " \n",
    "In this project, it is required to analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn = pd.read_csv(\"telecom_churn_data.csv\")\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data dictionary\n",
    "data_dict = pd.read_excel(\"Data+Dictionary-+Telecom+Churn+Case+Study.xlsx\")\n",
    "data_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting 5 random rows \n",
    "churn.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing data types in the data\n",
    "list(set(churn.dtypes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.shape\n",
    "print(\"There are {} rows and {} columns in data\".format(churn.shape[0],churn.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at number of unique values in columns\n",
    "churn.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the data as backup\n",
    "churn_original = churn.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of rows and columns at start\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First dropping the columns mobile_numer and circle_id as these wouldn't add any intelligence for modelling\n",
    "churn = churn.drop(['mobile_number', 'circle_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After dropping the above columns\n",
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = churn.isnull().sum()\n",
    "\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting overall idea as how many total missing values do we have.\n",
    "total_cells = np.product(churn.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing/total_cells) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMissingValues(data,range):\n",
    "    return sum([1 for d in data if d>range])\n",
    "\n",
    "# Finding the number of columns which are missing more than 70% values\n",
    "missing_values = (churn.isnull().sum() / churn.isnull().count() * 100 ).sort_values(ascending = False)\n",
    "findMissingValues(missing_values,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns with Object Types\n",
    "obj_df = churn.select_dtypes(include='O')\n",
    "obj_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the null values for Object data types\n",
    "missing_na_obj = (obj_df.isnull().sum() / obj_df.isnull().count() * 100 ).sort_values(ascending = False)\n",
    "missing_na_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the null values in date of last rech for months (6-9) \n",
    "churn['date_of_last_rech_6'].fillna('6/30/2014',inplace=True)\n",
    "churn['date_of_last_rech_7'].fillna('7/31/2014',inplace=True)\n",
    "churn['date_of_last_rech_8'].fillna('8/31/2014',inplace=True)\n",
    "churn['date_of_last_rech_9'].fillna('9/30/2014',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns with na values more than 70\n",
    "missing_na_obj_cols = list(missing_na_obj.loc[missing_na_obj > 70].index)\n",
    "missing_na_obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the above columns\n",
    "churn = churn.drop(columns = missing_na_obj_cols,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the remaining date(object) type columns\n",
    "date_cols_data = churn.select_dtypes(include='O')\n",
    "date_cols_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now, Imputing missing values by modeling each feature with missing values as a function \n",
    "#  of other features in a round-robin fashion.\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Excluding the date types(Object)\n",
    "df = churn.select_dtypes(exclude='O') \n",
    "# list(set(df.dtypes.tolist()))\n",
    "imp = IterativeImputer(max_iter=5, verbose=0)\n",
    "imp.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_churn = imp.transform(df)\n",
    "imputed_churn = pd.DataFrame(imputed_churn, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the values after imputing \n",
    "imported_churn_list = (imputed_churn.isnull().sum() / imputed_churn.isnull().count() * 100 ).sort_values(ascending = False)\n",
    "print(\"Missing values after imputing = {}\".format( sum(list(imported_churn_list.loc[imported_churn_list > 0].index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining back imputed and the date cols data\n",
    "churn = pd.concat([imputed_churn, date_cols_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns which are having constant value\n",
    "non_obj = churn.select_dtypes(exclude='O') \n",
    "const_cols = non_obj.columns[non_obj.nunique() <= 1]\n",
    "print (\"There are {} columns which has 1 unique value. These columns are: \\n{}\".format(len(const_cols),const_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove these columns as well, as all take constant values \n",
    "churn.drop(const_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into date cols for null values\n",
    "date_cols_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['last_date_of_month_7','last_date_of_month_8','last_date_of_month_9']\n",
    "for col in columns:\n",
    "    churn[col].fillna(churn[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking into date cols for null values after updating the values\n",
    "date_cols_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the data copy after cleansing\n",
    "churn_cleansed = churn.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filter high-value customers & derive new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.filter(regex='rech').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving New Feature :Total no. of data recharge based on 2g + 3g \n",
    "for i in range(6,10):\n",
    "    churn['total_rech_num_data_'+str(i)] = (churn['count_rech_2g_'+str(i)] +churn['count_rech_3g_'+str(i)]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving New Feature : Total recharge amount for data for months(6-9)\n",
    "for i in range(6,10):\n",
    "    churn['total_rech_amt_data_'+str(i)] = churn['total_rech_num_data_'+str(i)] * churn['av_rech_amt_data_'+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving New Feature :Total rechange amount for voice(calling) and Data for months(6-9)\n",
    "for i in range(6,10):\n",
    "    churn['total_month_rech_'+str(i)] = churn['total_rech_amt_'+str(i)]+churn['total_rech_amt_data_'+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.filter(regex=('total_month_rech')).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of first 2 months of good phase (Months 6 & 7) \n",
    "avg_good_phase = (churn['total_month_rech_6'] +churn['total_month_rech_7'] )/2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-value customers definition:\n",
    "-----------------------------------------------\n",
    "##### Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc = churn[avg_good_phase >=  np.percentile(avg_good_phase , 70)]\n",
    "hvc.reset_index(inplace=True , drop = True) \n",
    "print(\"High-value users {} with percentage {}% in data \".format(len(hvc), round(len(hvc)/churn.shape[0]*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating/Deriving some more new features based on the change between the 8th and avg of 6th and 7th month:\n",
    "\n",
    "# Recharge amount Voice\n",
    "hvc['total_rech_num_chng'] = hvc.total_rech_num_8 - ((hvc.total_rech_num_6 + hvc.total_rech_num_7)/2)\n",
    "hvc['total_rech_amt_chng'] = hvc.total_rech_amt_8 - ((hvc.total_rech_amt_6 + hvc.total_rech_amt_7)/2)\n",
    "hvc['max_rech_amt_chng'] = hvc.max_rech_amt_8 - ((hvc.max_rech_amt_6 + hvc.max_rech_amt_7)/2)\n",
    "\n",
    "# Recharge amount Data\n",
    "hvc['total_rech_data_chng'] = hvc.total_rech_data_8 - ((hvc.total_rech_data_6 + hvc.total_rech_data_7)/2)\n",
    "hvc['max_rech_data_chng'] = hvc.max_rech_data_8 - ((hvc.max_rech_data_6 + hvc.max_rech_data_7)/2)\n",
    "hvc['av_rech_amt_data_chng'] = hvc.av_rech_amt_data_8 - ((hvc.av_rech_amt_data_6 + hvc.av_rech_amt_data_7)/2)\n",
    "\n",
    "# OnNet and Offnet usage \n",
    "hvc['onnet_mou_chng'] = hvc.onnet_mou_8 - ((hvc.onnet_mou_6 + hvc.onnet_mou_7)/2)\n",
    "hvc['offnet_mou_chng'] = hvc.offnet_mou_8 - ((hvc.offnet_mou_6 + hvc.offnet_mou_7)/2)\n",
    "\n",
    "# roaming icoming /outgoing\n",
    "hvc['roam_ic_mou_chng'] = hvc.roam_ic_mou_8 - ((hvc.roam_ic_mou_6 + hvc.roam_ic_mou_7)/2)\n",
    "hvc['roam_og_mou_chng'] = hvc.roam_og_mou_8 - ((hvc.roam_og_mou_6 + hvc.roam_og_mou_7)/2)\n",
    "\n",
    "# Local calls usage\n",
    "hvc['loc_og_mou_chng'] = hvc.loc_og_mou_8 - ((hvc.loc_og_mou_6 + hvc.loc_og_mou_7)/2)\n",
    "hvc['loc_ic_mou_chng'] = hvc.loc_ic_mou_8 - ((hvc.loc_ic_mou_6 + hvc.loc_ic_mou_7)/2)\n",
    "\n",
    "# STD icoming/outgoing\n",
    "hvc['std_ic_mou_chng'] = hvc.std_ic_mou_8 - ((hvc.std_ic_mou_6 + hvc.std_ic_mou_7)/2)\n",
    "hvc['std_og_mou_chng'] = hvc.std_og_mou_8 - ((hvc.std_og_mou_6 + hvc.std_og_mou_7)/2)\n",
    "\n",
    "# ISD incoming/outgoing\n",
    "hvc['isd_ic_mou_chng'] = hvc.isd_ic_mou_8 - ((hvc.isd_ic_mou_6 + hvc.isd_ic_mou_7)/2)\n",
    "hvc['isd_og_mou_chng'] = hvc.isd_og_mou_8 - ((hvc.isd_og_mou_6 + hvc.isd_og_mou_7)/2)\n",
    "\n",
    "# Special calls incoming/outgoing\n",
    "hvc['spl_og_mou_chng'] = hvc.spl_og_mou_8 - ((hvc.spl_og_mou_6 + hvc.spl_og_mou_7)/2)\n",
    "hvc['spl_ic_mou_chng'] = hvc.spl_ic_mou_8 - ((hvc.spl_ic_mou_6 + hvc.spl_ic_mou_7)/2)\n",
    "\n",
    "# total incoming/outgoing\n",
    "hvc['total_og_mou_chng'] = hvc.total_og_mou_8 - ((hvc.total_og_mou_6 + hvc.total_og_mou_7)/2)\n",
    "hvc['total_ic_mou_chng'] = hvc.total_ic_mou_8 - ((hvc.total_ic_mou_6 + hvc.total_ic_mou_7)/2)\n",
    "\n",
    "# Vol 2G/3G MB \n",
    "hvc['vol_2g_mb_chng'] = hvc.vol_2g_mb_8 - ((hvc.vol_2g_mb_6 + hvc.vol_2g_mb_7)/2)\n",
    "hvc['vol_3g_mb_chng'] = hvc.vol_3g_mb_8 - ((hvc.vol_3g_mb_6 + hvc.vol_3g_mb_7)/2)\n",
    "\n",
    "# ARPU for voice and 2G/3G \n",
    "hvc['arpu_chng'] = hvc.arpu_8 - ((hvc.arpu_6 + hvc.arpu_7)/2)\n",
    "hvc['arpu_2g_chng'] = hvc.arpu_2g_8 - ((hvc.arpu_2g_6 + hvc.arpu_2g_7)/2)\n",
    "hvc['arpu_3g_chng'] = hvc.arpu_3g_8 - ((hvc.arpu_3g_6 + hvc.arpu_3g_7)/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tag churners and remove attributes of the churn phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase.\n",
    "###### Attributes to tag churners are: total_ic_mou_9 , total_og_mou_9 , vol_2g_mb_9 , vol_3g_mb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc['churn_flag'] = np.where(((hvc['total_ic_mou_9'] == 0.00) | (hvc['total_og_mou_9'] == 0.00)) & ((hvc['vol_2g_mb_9'] == 0.00)| (hvc['vol_3g_mb_9'] == 0.00)),1,0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"High-value Churn Percentage : {}%\".format(round(len(hvc[hvc.churn_flag == 1])/hvc.shape[0] *100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This indicates that the dataset is highly imbalanced data set where hurn cases are ~ 10 % as opposed to to the no-churn cases with the majority of ~ 90%\n",
    "\n",
    "Only when the class imbalance is high, e.g. 85%-90% points for one class and 10%-15% for the other, standard optimization criteria or performance measures may not be as effective and would need modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After tagging churners, remove all the attributes corresponding to the churn phase \n",
    "# (all attributes having ‘ _9’, etc. in their names).\n",
    "\n",
    "hvc = hvc.drop(hvc.filter(regex='_9|sep', axis= 1).columns, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "hvc['churn_flag'].value_counts().plot(kind = 'bar')\n",
    "plt.ylabel('Churn counts')\n",
    "plt.xlabel('Churn status')\n",
    "plt.title('Churn status report',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc.filter(regex='date').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date columns to dattime types\n",
    "for col in hvc.filter(regex='date'):\n",
    "    hvc[col]=pd.to_datetime(hvc[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days since recharge.\n",
    "for i in range(6,9):\n",
    "    hvc['days_since_recharge_'+str(i)] = (hvc[\"last_date_of_month_\"+str(i)] - hvc[\"date_of_last_rech_\"+str(i)]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the date columns after changing to other columns as above\n",
    "date_cols_to_drop = hvc.filter(regex='date').columns\n",
    "hvc = hvc.drop(columns=date_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hvc[hvc.columns[~hvc.filter(regex='since').isnull().values.any()]]\n",
    "#hvc[hvc.columns[~hvc.columns.isin(['churn_flag'])]]\n",
    "(hvc.isnull().sum() / hvc.isnull().count() * 100 ).sort_values(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBoxPlots(data):\n",
    "    for col in data.columns:\n",
    "        plt.figure()\n",
    "        plt.clf() \n",
    "        sns.boxplot(data[col],palette=\"deep\")\n",
    "        plt.title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawBoxPlots(hvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def outlier_treatment(datacolumn):\n",
    "#     sorted(datacolumn)\n",
    "#     Q1,Q3 = np.percentile(datacolumn , [25,75])\n",
    "#     IQR = Q3 — Q1\n",
    "#     lower_range = Q1 — (1.5 * IQR)\n",
    "#     upper_range = Q3 + (1.5 * IQR)\n",
    "#     return lower_range,upper_range\n",
    "\n",
    "# for col in hvc.columns:\n",
    "#     lowerbound,upperbound = outlier_treatment(hvc.col)\n",
    "#     outlier = hvc[(hvc.col < lowerbound) | (hvc.col > upperbound)]\n",
    "#     hvc.drop( outlier.index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cappingOutliers(data):\n",
    "    data = data.clip_upper(data.quantile(0.90))    \n",
    "    data = data.clip_lower(data.quantile(0.10))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc=hvc.apply(lambda x: cappingOutliers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvc_copy=hvc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = hvc.drop(['churn_flag'], axis=1)\n",
    "y = hvc['churn_flag']    \n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As there are large number of features,so we use PCA as dimensional reduction technique.\n",
    "# Rescaling the features before PCA as it is sensitive to the scales of the features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fitting and transforming the scaler on train\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "\n",
    "# transforming the train using the already fit scaler\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test),columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of churn before oversampling : {}\".format(sum(y_train==1)))\n",
    "print(\"Counts of no churn before oversampling : {}\".format(sum(y_train==0)))\n",
    "print(\"Churn rate before oversampling : {}% \\n\".format(round(sum(y_train==1)/len(y_train)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Handling imbalance using imblearn(SMOTE)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=100)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of churn after oversampling : {}\".format(sum(y_train_res==1)))\n",
    "print(\"Counts of no churn after oversampling : {}\".format(sum(y_train_res==0)))\n",
    "print(\"Churn rate after oversampling : {}% \\n\".format(round(sum(y_train_res==1)/len(y_train_res)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the PCA module\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(svd_solver='randomized', random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(X.columns)\n",
    "pca_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (12,10))\n",
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "# for i, txt in enumerate(pca_df.Feature):\n",
    "#     plt.annotate(txt, (pca_df.PC1[i],pca_df.PC2[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the screeplot - plotting the cumulative variance against the number of components\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca.explained_variance_ratio_: \",pca.explained_variance_ratio_.round(3)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot above and cumulative sum showed that ~ 60 components explains around 95% variance in the data set. \n",
    "# In order words, using PCA we have reduced 188 predictors to 60 without compromising on explained variance. \n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "pca_final = IncrementalPCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca_final.fit_transform(X_train_res)\n",
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating correlation matrix for the principal components\n",
    "corrmat = np.corrcoef(X_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the correlation matrix\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(corrmat,annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation should be close to zero\n",
    "\n",
    "corr_matrix_nod = corrmat - np.diagflat(corrmat.diagonal())\n",
    "print(\"max corr:\",corr_matrix_nod.max(), \", min corr: \", corr_matrix_nod.min(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying selected components to the test data\n",
    "X_test_pca = pca_final.transform(X_test)\n",
    "X_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction of churn customers we will be fitting variety of models and select one which is the best predictor of churn. Models used are as below:\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to display the sensitivity , specificity , accuracy of the model \n",
    "def disp_Metrics(target_test , target_pred):\n",
    "    confusion = confusion_matrix(target_test , target_pred)\n",
    "    TP = confusion[1,1] # true positive \n",
    "    TN = confusion[0,0] # true negatives\n",
    "    FP = confusion[0,1] # false positives\n",
    "    FN = confusion[1,0] # false negatives \n",
    "    \n",
    "    # Calculate false postive rate - predicting churn when customer does not have churned\n",
    "    print(\"False postive rate - predicting churn when customer does not have churned :{}\".format(round(FP/ float(TN+FP),2)))\n",
    "    print(\"Positive predictive value :{}\".format(round(TP/float(TP+FP),2)))\n",
    "    print(\"Negative predictive value :{}\".format(round(TN/float(TN+ FN),2)))\n",
    "    print(\"Sensitivity :{}\".format(round(TP/float(TP+FN),2)))\n",
    "    print(\"Specificity :{}\".format(round(TN/float(TN+FP),2)))\n",
    "    print(\"Accuracy :{}\".format(round(accuracy_score(target_test,target_pred),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function to draw ROC\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression on principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model on the train data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the algorithm on the data\n",
    "model_pca = lr.fit(X_train_pca, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the algorithm on the data\n",
    "y_pred_prob = model_pca.predict_proba(X_test_pca)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y_test to dataframe\n",
    "y_pred_df = pd.DataFrame(y_pred_prob)\n",
    "y_pred_1 = y_pred_df.iloc[:,[0]]\n",
    "y_pred_df.head()\n",
    "\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y_pred_final = pd.concat([y_test_df,y_pred_1],axis=1)\n",
    "y_pred_final= y_pred_final.rename(columns={ 0 : 'churn_prob'})\n",
    "\n",
    "y_pred_final['predicted'] = y_pred_final.churn_prob.map( lambda x: 1 if x > 0.5 else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_pred_final[i]= y_pred_final.churn_prob.map( lambda x: 1 if x > i else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix( y_pred_final.churn_flag, y_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    sensi = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    speci = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above optimal cutoff diagram taking prob as 0.5\n",
    "cutoff = 0.5\n",
    "y_pred= [1 if proba > cutoff else 0 for proba in y_pred_prob]\n",
    "print('AUC:', round(roc_auc_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying metrics\n",
    "disp_Metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing ROC\n",
    "draw_roc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing decision tree classifier from sklearn library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced',\n",
    "                             max_features='auto',\n",
    "                             min_samples_split=100,\n",
    "                             min_samples_leaf=100,\n",
    "                             max_depth=6,\n",
    "                             random_state=10)\n",
    "dt.fit( X_train_pca, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = dt.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df, annot=True,annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_Metrics(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid \n",
    "param_grid = {\n",
    "    'max_depth': range(5, 15, 5),\n",
    "    'min_samples_leaf': range(50, 150, 50),\n",
    "    'min_samples_split': range(50, 150, 50),\n",
    "    'criterion': [\"entropy\", \"gini\"]\n",
    "}\n",
    "\n",
    "n_folds = 5\n",
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier(class_weight='balanced')\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n",
    "                          cv = n_folds, verbose = 1,n_jobs=-1,scoring='recall')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_pca, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = grid_search.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "dtree = DecisionTreeClassifier(class_weight='balanced',max_depth=10,min_samples_leaf=50,min_samples_split=50)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "dtree.fit(X_train_pca, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = dtree.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the heatmap\n",
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df, annot=True,annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "disp_Metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_auc_score\n",
    "pred_test = dtree.predict_proba(X_test_pca)[:,1]\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, pred_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest on PCA dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "rfc.fit(X_train_pca,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test data\n",
    "y_pred = rfc.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df, annot=True,annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "disp_Metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = rfc.predict_proba(X_test_pca)[:,1]\n",
    "round(metrics.roc_auc_score(y_test, pred_test),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [4,8,10],\n",
    "    'min_samples_leaf': range(100, 400, 200),\n",
    "    'min_samples_split': range(200, 500, 200),\n",
    "    'n_estimators': [100,200, 300], \n",
    "    'max_features': [5, 10]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1,verbose = 1,scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_pca, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall\n",
    "print(grid_search.best_score_ )\n",
    "#optimal accuracy\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=200,\n",
    "                             max_features=5,\n",
    "                             n_estimators=100\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the algorithm on the data\n",
    "rfc.fit(X_train_pca,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = rfc.predict(X_test_pca)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df, annot=True,annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "disp_Metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test = rfc.predict_proba(X_test_pca)[:,1]\n",
    "metrics.roc_auc_score(y_test, pred_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the Logistic Regression model with probability performs best. It achieved the best recall accuracy of 83% for test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree based model to identify the important predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier(class_weight={0:0.11,1:0.89})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df, annot=True,annot_kws={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "disp_Metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test = rfc.predict_proba(X_test)[:,1]\n",
    "metrics.roc_auc_score(y_test, pred_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity is poor for the default model. Let's try tuning the hypeparameters to improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [4,8,10],\n",
    "    'min_samples_leaf': range(100, 300, 100),\n",
    "    'min_samples_split': range(200, 400, 100),\n",
    "    'n_estimators': [100,200,300,500], \n",
    "    'max_features': [5,10]\n",
    "}\n",
    "# Create a Random Forest based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1,verbose = 1,scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_depth=10,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=300,\n",
    "                             max_features=10,\n",
    "                             n_estimators=500,\n",
    "                             class_weight={0:0.11,1:0.89})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(df, annot = True, annot_kws = {\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "disp_Metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_test = rfc.predict_proba(X_test)[:,1]\n",
    "metrics.roc_auc_score(y_test, pred_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model metrics are quite satisfactory. Let's now proceed with identifying important predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the name and gini importance of each feature\n",
    "features = X_train.columns\n",
    "for feature in zip(features, rfc.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.concat([pd.Series(features),pd.Series(rfc.feature_importances_)],axis=1).rename(columns={0:'Features',1:'Weight'})\n",
    "feature_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 features\n",
    "top_20 = feature_df.sort_values('Weight',ascending=False).head(20)\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = sns.barplot(data = top_20, x='Features', y='Weight',edgecolor=sns.color_palette('dark',5))\n",
    "ax.set_title(\"Top 20 Churn Predictors\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_byChurn(col,colList):\n",
    "    # per month churn vs Non-Churn\n",
    "    fig, ax = plt.subplots(figsize=(7,4))\n",
    "    plt.plot(hvc.groupby('churn_flag')[colList].mean().T)\n",
    "    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n",
    "    ## Add legend\n",
    "    plt.legend(['No Churn', 'Churn'])\n",
    "    # Add titles\n",
    "    plt.title(col, loc='left', fontsize=14, fontweight=0, color='blue')\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_plot(data):\n",
    "    sns.set(style=\"ticks\", color_codes=True)\n",
    "    hue = 'churn_flag'\n",
    "    sns.pairplot(data,hue,vars=data.iloc[:,:3],size=3, palette=\"husl\",markers=[\"o\", \"s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_col = ['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8']\n",
    "plot_byChurn(\"total_ic_mou\",ic_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_og_mou_col =  ['total_og_mou_6' ,  'total_og_mou_7' ,  'total_og_mou_8']\n",
    "plot_byChurn( total_og_mou_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_og_mou_col =  ['loc_og_mou_6' ,  'loc_og_mou_7' ,  'loc_og_mou_8']\n",
    "plot_byChurn(\"loc_og_mou\", loc_og_mou_col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rech_amt_col = ['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8']\n",
    "plot_byChurn(\"total_rech_amt\",total_rech_amt_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roam_og_mou_col = ['roam_og_mou_6', 'roam_og_mou_7','roam_og_mou_8']\n",
    "plot_byChurn(\"roam_og_mou\",roam_og_mou_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roam_ic_mou_col = ['roam_ic_mou_6', 'roam_ic_mou_7','roam_ic_mou_8']\n",
    "plot_byChurn(\"roam_ic_mou_6\",roam_ic_mou_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpu_col = ['arpu_6', 'arpu_7','arpu_8']\n",
    "plot_byChurn(\"arpu\",arpu_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "hvc['churn_flag'].value_counts().plot(kind = 'bar')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Churn')\n",
    "plt.title('Churn Distribution',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Revenue per user \n",
    "data=hvc[['arpu_6' , 'arpu_7' , 'arpu_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage within same network\n",
    "data = hvc[['onnet_mou_6' , 'onnet_mou_7' , 'onnet_mou_8' , 'churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage outside of the operator network\n",
    "data=hvc[['offnet_mou_6' ,'offnet_mou_7' ,'offnet_mou_8' ,'churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local outgoing Calls\n",
    "data= hvc[['loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD outgoing calls\n",
    "data= hvc[['std_og_mou_6','std_og_mou_7','std_og_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISD outgoing calls\n",
    "data=hvc[['isd_og_mou_6','isd_og_mou_7','isd_og_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total outgoing calls\n",
    "data=hvc[['total_og_mou_6','total_og_mou_7','total_og_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roaming outgoing Calls\n",
    "data= hvc[['roam_og_mou_6','roam_og_mou_7','roam_og_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incoming Roaming Calls\n",
    "data= hvc[['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local incoming calls\n",
    "data=hvc[['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD incoming calls\n",
    "data=hvc[['std_ic_mou_6','std_ic_mou_7','std_ic_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total incoming calls\n",
    "data=hvc[['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total recharge numbers for voice\n",
    "data=hvc[['total_rech_num_6','total_rech_num_7','total_rech_num_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total recharge amount for voice\n",
    "data=hvc[['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2G data usage\n",
    "data= hvc[['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3G data usage\n",
    "data= hvc[['vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume based\n",
    "data= hvc[['jun_vbc_3g','jul_vbc_3g','aug_vbc_3g','churn_flag']]\n",
    "pair_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Indicators/Recommendations/strategies\n",
    "\n",
    "It is seen and clear from the Feature importance section, that action phase(Month 8th) has most significant impact on the customer churn of high value customers, and thus focussing on these features forms the clear indication for the churn.Overall, a reduction in any of these indicator KPIs suggest that customer is not fulling partaking in the services offered and thus may potentially churn in the near future.\n",
    "\n",
    "It is also understood from above that incoming & outgoing calls(both roaming and local) are key for identifying churn customers, and this is especially important during the action phase in comparison to the 6th and 7th month(Good phase), since these are observed to be reduced.\n",
    "\n",
    "The other important feature(factor) is the recharge amount which also showed a dip in the action phase(8th month).The average revenue per user has also observed decline when compared with 7th month, so ARPU is also another important consideration business should be looking into to avoid churn.\n",
    "\n",
    "Overall business sholud atleast be closely monitoring these important features on more short duration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
